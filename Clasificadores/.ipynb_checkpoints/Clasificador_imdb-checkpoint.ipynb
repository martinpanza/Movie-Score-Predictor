{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer Intento de clasificador\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el titulo lo dice, primer intento de clasificador usando como referencia el laboratorio x .\n",
    "Se considera para este caso el archivo movies_final_imdb el cual contiene:\n",
    "    \n",
    "    movieId+nota_imbd_aprox+duracion+categoria+actores+directores\n",
    "    \n",
    "    \n",
    "Nota : Falta agregar los directores y quizas las fechas para ve como cambia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Librerias Basicas:\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Metricas y Estadisticas:\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "# Clasificadores : Tree, SVM, RandomForest, Regression, Naive Bayes,KNN......\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"movies_final.csv\", header = 0)\n",
    "score = pd.read_csv(\"imdb.csv\", header = 0)\n",
    "\n",
    "movies_header = list(movies.columns.values)\n",
    "movies = movies.as_matrix()\n",
    "\n",
    "score = np.ravel(score.as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificadores y Validación.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Esta nueva seccion calcula el promedio del accuracy de varios tipos de clasificadores y despues corre un test estadistico para determinar si existe una diferencia significativa entre el clasificador que se está probando y los otros.\n",
    "\n",
    "~Algo asi como un cross validation.\n",
    "\n",
    "\n",
    "Definimos las funciones del clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variar el num_test modifica la probabilidad de estar mas cerca del promedio al hacer un t-test. \n",
    "#Creo que con 35 es suficiente para ese tipo de distribuciones.\n",
    "\n",
    "def run_classifier(clf, X, y, num_tests=10):\n",
    "    scores = []\n",
    "    reports = []\n",
    "    \n",
    "    for i in range(num_tests):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificadores usados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c0 = (\"Base\", DummyClassifier(strategy='stratified'))\n",
    "c1 = (\"SVM\", SVC(kernel='linear'))\n",
    "c2 = (\"DT\", DecisionTreeClassifier())\n",
    "c3 = (\"NB\", GaussianNB())\n",
    "c4 = (\"KNN\", KNeighborsClassifier(n_neighbors=5))\n",
    "c5 = (\"RandomForest\", RandomForestClassifier(n_estimators=10))\n",
    "c6 = (\"LogisticRegression\",LogisticRegression())\n",
    "\n",
    "classifiers = [c0, c1, c2, c3, c4,c5,c6]\n",
    "result_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos las pruebas para cada test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Funcion Pesada, correr solo una vez. \n",
    "\n",
    "for name, clf in classifiers:\n",
    "    accuracys = run_classifier(clf ,movies , score)\n",
    "    result_list.append((name, accuracys))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Impresion de los resultados\n",
    "\n",
    "print(\"+ indica diferencia significativa\\n\")\n",
    "\n",
    "for name1, results1 in result_list:\n",
    "    print(\"Comparando %s - Accuracy: %.2f\" % (name1, results1.mean()))\n",
    "    for name2, results2 in result_list:\n",
    "        if name1 == name2:\n",
    "            continue\n",
    "\n",
    "        _, p_value, __ = ttest_ind(results1, results2)\n",
    "        \n",
    "        if p_value <= 0.05:\n",
    "            sig = \"+\"\n",
    "        else:\n",
    "            sig = \"\"\n",
    "        print(\"%s:\\t%.2f %s\" % (name2, results2.mean(), sig))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones de algunas peliculas\n",
    "\n",
    "Usando la siguiente funcion, se puede imprimir mas facilmente los resultados y luego calcular algunas metricas importantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for clf_name, clf in classifiers:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(movies, score, test_size =  0.30)\n",
    "    clf.fit(X_train, y_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_movies = [(\"Toy Story\", movies[0], score[0]),(\"Avatar\", movies[3111], score[3111]),\n",
    "                 (\"Titanic\",movies[458],score[458]),(\"Transformers\",movies[4057],score[4057]),\n",
    "                 (\"Never Say Never\",movies[4189],score[4189])]\n",
    "predicted_scores = []\n",
    "\n",
    "# Toma un vector de tuplas nombre,clasificador y otro de nombre_pelicula,atributos,score_original y\n",
    "# trata de predecir el score de esto para luego compararlos.\n",
    "def predict_scores(clf, sample_movies):    \n",
    "    for clf_name , clf in clf:\n",
    "        for sample_name, sample_data, sample_score in sample_movies:\n",
    "            predicted_scores.append((clf_name, sample_name,clf.predict(sample_data),sample_score))\n",
    "    return predicted_scores\n",
    "\n",
    "res = predict_scores(classifiers, sample_movies)\n",
    "\n",
    "def predict_scores_toString(predicted_scores):\n",
    "    string = \"Clf\\t\\t\\tNombrePelicula\\t\\t\\tNotaPredicha\\t\\tNotaReal\\n\"\n",
    "    for item in predicted_scores:\n",
    "        for elements in item:\n",
    "            string = str(string) + str(elements) + \"\\t\\t\\t\"\n",
    "        string = str(string) + \"\\n\"\n",
    "    return string\n",
    "        \n",
    "#Muchos Warnings :(        \n",
    "\n",
    "res2 = predict_scores_toString(res)\n",
    "print res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda del mejor train test\n",
    "\n",
    "A traves de graficos, trataremos de buscar el mejor test-train para cada clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_intervals(number):\n",
    "    intervals = []\n",
    "    for i in range(number):\n",
    "        intervals.append(float(i)/number)\n",
    "    \n",
    "    return intervals[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Funcion que determina la mejor proporcion del conjunto de entrenamiento \n",
    "# y retorna la lista mas el mejor clasificador generado.\n",
    "def best_test_size(clf, X, y, intervals, test_iterations):\n",
    "    scores_final = []\n",
    "    scores = []\n",
    "    best_clf = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for i in intervals:\n",
    "        for j in range(test_iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i)\n",
    "            clf.fit(X_train, y_train)\n",
    "            clf_score = clf.score(X_test, y_test)\n",
    "            scores.append(clf_score)\n",
    "            if (best_accuracy < clf_score):\n",
    "                best_accuracy = clf_score\n",
    "                best_clf = clf\n",
    "            \n",
    "            \n",
    "        \n",
    "        #Agrego el promedio de los resultados.\n",
    "        scores_final.append(sum(scores) / float(len(scores)))\n",
    "        \n",
    "        #Vacio las listas.\n",
    "        scores = []\n",
    "            \n",
    "    print scores_final  \n",
    "    return (np.array(scores_final),best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "intervalos1 = 10\n",
    "\n",
    "test_size_results = []\n",
    "intervals = create_intervals(intervalos1)\n",
    "\n",
    "for clf_name, clf in classifiers:\n",
    "    print \"CLF Actual: \" + clf_name\n",
    "    test_results = best_test_size (clf, movies, score, 10,2) \n",
    "    #print test_results\n",
    "    plt.plot(create_intervals(10),test_results)\n",
    "    plt.ylabel('Accuracy de '+ clf_name)\n",
    "    plt.show()\n",
    "    test_size_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Conclusion. Aparentemente, SVM con Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejor Parametro KNN (numero de vecinos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_parameter_KNN(X, y,parameters,tsize):\n",
    "    scores = []\n",
    "    param = []\n",
    "    \n",
    "    for i in range(parameters):\n",
    "        param.append(i+1)\n",
    "        for j in range(tsize):\n",
    "            results = []\n",
    "            clf = KNeighborsClassifier(n_neighbors=i+1)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "            clf.fit(X_train, y_train)\n",
    "            results.append(clf.score(X_test, y_test))\n",
    "        \n",
    "        scores.append(sum(results) / float(len(results)))\n",
    "        results = []\n",
    "    #Imprimo el grafico!\n",
    "                      \n",
    "    print 'Accuracy de KNN con ' + str(parameters) + ' parametros y ' + str(tsize) + ' iteraciones x parametro.'\n",
    "    print param\n",
    "    plt.plot(param,scores)\n",
    "    plt.ylabel('Accuracy de KNN') \n",
    "    plt.xlabel('Numero de Parametros')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "best_parameter_KNN(movies,score,30,15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mejor Parametro RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_parameter_RF(X, y,parameters,tsize):\n",
    "    scores = []\n",
    "    param = []\n",
    "    \n",
    "    for i in range(parameters):\n",
    "        param.append(i+1)\n",
    "        for j in range(tsize):\n",
    "            results = []\n",
    "            clf = RandomForestClassifier(n_estimators=i+1)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "            clf.fit(X_train, y_train)\n",
    "            results.append(clf.score(X_test, y_test))\n",
    "        \n",
    "        scores.append(sum(results) / float(len(results)))\n",
    "        results = []\n",
    "    #Imprimo el grafico!\n",
    "                      \n",
    "    print 'Accuracy de KNN con ' + str(parameters) + ' parametros y ' + str(tsize) + ' iteraciones x parametro.'\n",
    "    print param\n",
    "    plt.plot(param,scores)\n",
    "    plt.ylabel('Accuracy de KNN') \n",
    "    plt.xlabel('Numero de Parametros')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "best_parameter_RF(movies,score,30,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2\n",
    "\n",
    "Para llegar a este test se hizo lo siguiente :\n",
    "\n",
    "    1.Se determino en el test1 cuales eran los clasificadores que daban mejores resultados.\n",
    "\n",
    "    2.Se determino (solo para 0.30 el tamaño del dataset) cuales eran los mejores parametros para random forest tree y KNN.\n",
    "\n",
    "Aun falta :\n",
    "\n",
    "Determinar los mejores resultados (usando algun algoritmo greedy) cual es el optimo de parametros para knn y random forest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test 2 . Parametros :\n",
    "\n",
    "#Intervalos por probar\n",
    "intervals2 = [0.1,0.2,0.3,0.4,0.5]\n",
    "#Numero de iteraciones probadas.\n",
    "iteraciones2 = 1\n",
    "\n",
    "\n",
    "test_size_results = []\n",
    "\n",
    "\n",
    "c00 = (\"Base\", DummyClassifier(strategy='stratified'))\n",
    "c11 = (\"SVM\", SVC(kernel='linear'))\n",
    "c22 = (\"KNN\", KNeighborsClassifier(n_neighbors=30))\n",
    "c33 = (\"LogisticRegression\",LogisticRegression())\n",
    "\n",
    "classifiers2 = [c00, c11, c22, c33]\n",
    "best_classifiers = []\n",
    "\n",
    "for clf_name, clf in classifiers2:\n",
    "    test_results2 = best_test_size (clf, movies, score, intervals2,iteraciones2) \n",
    "    best_classifiers.append(test_results2[1])\n",
    "    \n",
    "    #print test_results\n",
    "    plt.plot(intervals2,test_results2[0])\n",
    "    plt.ylabel('Accuracy de '+ clf_name)\n",
    "    plt.show()\n",
    "    test_size_results2 = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imprimo los classification report de los mejores clasificadores que se obtuvieron. \n",
    "clf_names = [\"Dummy:\\n\", \"SVM:\\n\", \"KNN:\\n\", \"Logistic Regression:\\n\"]\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "i = 0\n",
    "for clf in best_classifiers:\n",
    "    print clf_names[i]\n",
    "    score_pred = OneVsRestClassifier(clf.predict(movies))\n",
    "    print(classification_report(score,score_pred))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas :\n",
    "    Clasificadores Pipeline : Peliculas (mala,buena) -> Peliculas (malas, medias, buenas) -> Peliculas(muy malas,malas,medias,buenas,muy buenas).\n",
    "\n",
    "Que porcentaje de peliculas no tiene suficientes metadatos. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificadores Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cargamos el archivo imbd_mayor3 que tiene 2 clases : 1 si la nota es mayor que 3 y 0 si no.\n",
    "\n",
    "scores_greater3 = pd.read_csv(\"imdb_mayor3.csv\", header = 0)\n",
    "scores_greater3 = np.ravel(scores_greater3.as_matrix())\n",
    "\n",
    "greater3_clf = LogisticRegression()\n",
    "scores = cross_validation.cross_val_score( greater3_clf,movies , scores_greater3, cv=15)\n",
    "print scores\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(movies, scores_greater3, test_size = 0.30)\n",
    "greater3_clf.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(scores_greater3,greater3_clf.predict(movies)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbol de clasificadores \n",
    "    \n",
    "    Tratamos de asignarle sentido a lo que estamos clasificando a través de varios clasificadores ordenados en un arbol.\n",
    "   \n",
    "Usaremos Regresion Logistica.   \n",
    "   \n",
    "Aun resta por ver Clasificador OneVSAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4452 4452\n",
      "612673 227\n",
      "4452 4452\n",
      "9343938 3462\n",
      "2059337 763\n"
     ]
    }
   ],
   "source": [
    "#Definiciones Previas\n",
    "\n",
    "#Funciones\n",
    "def load_archive(name):\n",
    "    archive = pd.read_csv(name, header = 0)\n",
    "    archive = np.ravel(archive.as_matrix())\n",
    "    return archive\n",
    "\n",
    "#Clasificadores\n",
    "clf_cmp4 = LogisticRegression() # 0 si es menor que 4, 1 de lo contrario.\n",
    "clf_1to4 = LogisticRegression() # Clasifica en un rango de notas de 1 a 4.\n",
    "clf_cmp7 = LogisticRegression() # 0 si es menor que 7, 1 de lo contrario\n",
    "clf_5to7 = LogisticRegression() # Clasifica de 4 a 7. \n",
    "clf_8to10 = LogisticRegression() # Clasifica de 8 a 10.\n",
    "\n",
    "#Archivos y Arrays.\n",
    "\n",
    "movies_0to4 = load_archive(\"movies_0to4.csv\")\n",
    "movies_5to7 = load_archive(\"movies_5to7.csv\")\n",
    "movies_8to10 = load_archive(\"movies_8to10.csv\")\n",
    "\n",
    "scores_cmp4 = load_archive(\"scores_cmp4.csv\")\n",
    "scores_0to4 = load_archive(\"scores_0to4.csv\")\n",
    "scores_cmp7 = load_archive(\"scores_cmp7.csv\")\n",
    "scores_5to7 = load_archive(\"scores_5to7.csv\")\n",
    "scores_8to10 =load_archive(\"scores_8to10.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Arrays Listos\n",
    "classifiersTree = [clf_cmp4,clf_1to4,clf_cmp7,clf_5to7,clf_8to10 ] \n",
    "movies_datasets = [movies, movies_0to4, movies,movies_5to7,movies_8to10]\n",
    "scores_datasets = [scores_cmp4, scores_0to4, scores_cmp7 ,scores_5to7,scores_8to10]\n",
    "\n",
    "for i in range(len(classifiersTree)):\n",
    "    print str(len(movies_datasets[i]))+\" \" + str(len(scores_datasets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [ 227 4452]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b80f3721ee7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclassifiersTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mclassifiersTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiersTree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmovies_datasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b80f3721ee7a>\u001b[0m in \u001b[0;36mtrain_trees\u001b[0;34m(classifiersTree, movies, score_datasets)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrained_classifiersTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiersTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mclassifiersTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrained_classifiersTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiersTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[0;32m/home/pablo/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [ 227 4452]"
     ]
    }
   ],
   "source": [
    "#Funciones.\n",
    "\n",
    "#Entrena a los clasificadores usando los dataset correspondientes.\n",
    "def train_trees(classifiersTree, movies, score_datasets):\n",
    "    trained_classifiersTree = []\n",
    "    for i in range(len(classifiersTree)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(movies_datasets[0], scores_datasets[1], test_size = 0.5)\n",
    "        classifiersTree[i].fit(X_train, y_train)\n",
    "        trained_classifiersTree.append(classifiersTree[i])\n",
    "    return trained_classifiersTree\n",
    "    \n",
    "\n",
    "#Predice y evalua.\n",
    "def predict_with_clfTree(value_to_predict, classifiersTree):\n",
    "    \n",
    "    #Clasifico si la nota es mayor o menor que 4.\n",
    "    if (classifiersTree[0].predict(value_to_predict)):\n",
    "        #Clasifico si la nota es mayor o menor que 7\n",
    "        if (classifiersTree[2].predict(value_to_predict)):\n",
    "            return classifiersTree[4].predict(value_to_predict)\n",
    "            #Si es menor clasifico \n",
    "        else:\n",
    "            return classifiersTree[3].predict(value_to_predict)\n",
    "        \n",
    "    #Si es menor que 4: \n",
    "    else:\n",
    "        return classifiersTree[1].predict(value_to_predict)\n",
    "    \n",
    "classifiersTree = train_trees(classifiersTree,movies_datasets,scores_datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A ver que tal funciona!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
